7.2.2 Single-Step Datasets
As a step towards fully conversational systems, a number of challenges
have been proposed to address the necessary sub-tasks. Here we refer to
them as single-step datasets, as the focus is on a single step within the
many that a conversational system must perform. We note that they do
not focus on single dialogue turns (as is the case with Conversational QA
datasets), but even more fundamental steps of information processing.
One recent example is generating the natural text from structured
information to describe a particular search result, as the conversational
equivalent of search snippet generation (Turpin et al., 2007). For
instance, suppose a conversational agent needs to explain a specific
restaurant to a user, showing how it satisfies their request. The agent
may possess rich structured information about the restaurant ‚Äì its
name, address, the type of food offered, pricing information, and other
key attributes. However, just presenting these facets of information to
the user may not be suitable. The End-to-End NLG Challenge (Du≈°ek
et al., 2018) produced a dataset mapping a set of attributes to natural
language descriptions, allowing a challenge for generating text from
structured information ‚Äì a critical single step of many CIS systems.
A second example where single-step datasets are used is for applications
where generating full text is unnecessary. This common task
treats conversational information seeking as the ranking of possible
(existing) responses that an agent could give at a particular time. For
instance, Yang et al. (2018a) described datasets derived from transcripts
of past technical support dialogues: They assume that for any given
user utterance, the system should select from previous agent utterances
(as most technical support problems are not novel). Such specialized
single-step datasets will address this single-turn ranking problem.
As a third example, when an agent asks a question, it must be able
to interpret the user‚Äôs answers. Taking the seemingly simple case of
yes/no questions, a user may answer indirectly. For instance, if an agent
asks if a user would be interested in an evening activity, the user may
say ‚ÄúI‚Äôd prefer to go to bed‚Äù rather than simply ‚Äúno‚Äù. The Circa dataset
(Louis et al., 2020) was developed to contain natural questions and
answers to train and evaluate reliable answer interpretation by CIS
systems. The approach used multiple phases of crowdworker tasks first
to develop natural questions and then, in turn, natural answers while
attempting to minimize bias and maximize the diversity and naturalness
of answers.

7.2.3 Simulated Users
A recent alternative to static conversational datasets is relying on simulators
(Zhang and Balog, 2020; Ie et al., 2019; Aliannejadi et al., 2021a).
For instance, Zhang and Balog (2020) argued that a simulator ‚Äúshould
enable to compute an automatic assessment of the agent such that it
is predictive of its performance with real users‚Äù. In this way, rather
than evaluating with a fixed dataset, an agent could be assessed dynamically
against a (fixed) simulator to obtain the benefits of effective
offline evaluation. This recent work showed a high correlation between
simulation-based evaluation and an online evaluation approach. Simulation
also addresses challenges in fixed datasets, particularly relating to
user privacy (Slokom, 2018).

7.4.1 Metrics for Individual Steps
At individual steps, it is possible to evaluate whether the system understood
a user‚Äôs utterance, whether the search system respected a
constraint, or whether a system utterance was fluent among other
things. Most often, these aspects can be measured with metrics that
can be computed offline.
As an example, we take conversational question answering (ConvQA),
discussed in depth in Section 5.1. Often used to assess clarification
approaches in the NLP community (Rao and Daum√© III, 2019), common
metrics include BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), and
METEOR (Banerjee and Lavie, 2005). At a high level, these metrics
match the similarity between a given string and reference strings. While
effective for some applications, these metrics do not correlate highly
with user satisfaction in conversational systems (Liu et al., 2016). More
recently, machine learned metrics have achieved significantly higher
correlation with manual human ratings for such language tasks (Ma
et al., 2018; Sellam et al., 2020).
When assessing the relevance of recommendations that terminate
a conversational exchange, classic information retrieval metrics are
used (Croft et al., 2010). For instance, Normalized Discounted Cumulative
Gain (nDCG), Mean Reciprocal Rank (MRR), and Precision are
often used to assess if recommended items match information needs
of users given a particular user representation, e.g., (Christakopoulou
et al., 2016), if a system is able to rank possible clarifying question,
e.g., (Aliannejadi et al., 2019), or if a system accurately provides answers
to specific requests, e.g., (Christmann et al., 2019). As with language
metrics, such metrics do not necessarily agree with user experience of
an end-to-end system (Jiang and Allan, 2016).
As an example of more nuanced refinements of relevance in a conversational
setting, consider work by Rashkin et al. (2021). Here, the
authors propose a metric that assesses whether a CIS system only
presents verifiable information, rather than hallucinated or factually
unverifiable information.

cently
developed. Some of them have already been widely used in commercial systems.
 Dialogue systems for question answering, task completion, chitchat and recommendation
etc. can be conceptualized using a unified mathematical framework of optimal decision
process. The neural approaches to AI, developed in the last few years, leverage the recent
breakthrough in RL and DL to significantly improve the performance of dialogue agents
across a wide range of tasks and domains.
 A number of commercial dialogue systems allow users to easily access various services and
information via conversation. Most of these systems use hybrid approaches that combine
the strength of symbolic methods and neural models.
 There are two types of QA agents. KB-QA agents allow users to query large-scale knowledge
bases via conversation without composing complicated SQL-like queries. Text-QA
agents, equipped with neural MRC models, are becoming more popular than traditional
search engines (e.g., Bing and Google) for the query types to which users expect a concise
direct answer.
 Traditional task-oriented systems use handcrafted dialogue manager modules, or shallow
machine-learning models to optimize the modules separately. Recently, researchers have
begun to explore DL and RL to optimize the system in a more holistic way with less domain
knowledge, and to automate the optimization of systems in a changing environment such
that they can efficiently adapt to different tasks, domains and user behaviors.
 Chatbots are important in facilitating smooth and natural interaction between humans and
their electronic devices. More recent work focuses on scenarios beyond chitchat, e.g.,
recommendation. Most state-of-the-art chatbots use fully data-driven and end-to-end generation
of conversational responses within the framework of neural machine translation.


Recent years have witnessed an increasing demand for conversational Question Answering (QA)
agents that allow users to query a large-scale Knowledge Base (KB) or a document collection in
natural language. The former is known as KB-QA agents and the latter text-QA agents. KB-QA
agents are more flexible and user-friendly than traditional SQL-like systems in that users can query
a KB interactively without composing complicated SQL-like queries. Text-QA agents are much
easier to use in mobile devices than traditional search engines, such as Bing and Google, in that they
provide concise, direct answers to user queries, as opposed to a ranked list of relevant documents.

Conversational KB-QA Agents
All of the KB-QA methods we have described so far are based on single-turn agents which assume
that users can compose in one shot a complicated, compositional natural language query that can
uniquely identify the answer in the KB.
However, in many cases, it is unreasonable to assume that users can construct compositional queries
without prior knowledge of the structure of the KB to be queried. Thus, conversational KB-QA
agents are more desirable because they allow users to query a KB interactively without composing
complicated queries.
A conversational KB-QA agent is useful for many interactive KB-QA tasks such as movie-ondemand,
where a user attempts to find a movie based on certain attributes of that movie, as illustrated
by the example in Fig. 3.4, where the movie DB can be viewed as an entity-centric KB consisting
of entity-attribute-value triples.
In addition to the core KB-QA engine which typically consists of a semantic parser and a KBR
engine, a conversational KB-QA agent is also equipped with a Dialogue Manager (DM) which
tracks the dialogue state and decides what question to ask to effectively help users navigate the KB
in search of an entity (movie). The high-level architecture of the conversational agent for movieon-
demand is illustrated in Fig. 3.5. At each turn, the agent receives a natural language utterance ut
as input, and selects an action at 2 A as output. The action space A consists of a set of questions,
each for requesting the value of an attribute, and an action of informing the user with an ordered
list of retrieved entities. The agent is a typical task-oriented dialogue system of Fig. 1.2 (Top),
consisting of (1) a belief tracker module for resolving coreferences and ellipsis in user utterances
using conversation context, identifying user intents, extracting associated attributes, and tracking the
dialogue state; (2) an interface with the KB to query for relevant results (i.e., the Soft-KB Lookup
component, which can be implemented using the KB-QA models described in the previous sections, except that we need to form the query based on the dialogue history captured by the belief tracker,
not just the current user utterance, as described in Suhr et al. (2018)); (3) a beliefs summary module
to summarize the state into a vector; and (4) a dialogue policy which selects the next action based on
the dialogue state. The policy can be either programmed (Wu et al., 2015) or trained on dialogues
(Wen et al., 2017; Dhingra et al., 2017).

Conversational Text-QA Agents
While all the neural MRC models described in Sec. 3.7 assume a single-turn QA setting, in reality,
humans often ask questions in a conversational context (Ren et al., 2018a). For example, a user
might ask the question ‚Äúwhen was California founded?‚Äù, and then depending on the received answer,
follow up by ‚Äúwho is its governor?‚Äù and ‚Äúwhat is the population?‚Äù, where both refer to ‚ÄúCalifornia‚Äù
mentioned in the first question. This incremental aspect, although making human conversations
succinct, presents new challenges that most state-of-the-art single-turn MRC models do not address
directly, such as referring back to conversational history using coreference and pragmatic reasoning10
(Reddy et al., 2018).
A conversational text-QA agent uses a similar architecture to Fig. 3.5, except that the Soft-KB
Lookup module is replaced by a text-QA module which consists of a search engine (e.g., Google


Evaluation Metrics
While individual components in a dialogue system can often be optimized against more well-defined
metrics such as accuracy, precision, recall, F1 and BLEU scores, evaluating a whole dialogue system
requires a more holistic view and is more challenging (Walker et al., 1997, 1998, 2000; Paek,
2001; Hartikainen et al., 2004). In the reinforcement-learning framework, it implies that the reward
function has to take multiple aspects of dialogue quality into consideration. In practice, the reward
function is often a weighted linear combination of a subset of the following metrics.
The first class of metrics measures task completion success. The most common choice is perhaps
task success rate‚Äîthe fraction of dialogues that successfully solve the user‚Äôs problem (buying the
right movie tickets, finding proper restaurants, etc.). Effectively, the reward corresponding to this
metric is 0 for every turn, except for the last turn where it is +1 for a successful dialogue and ÙÄÄÄ1
otherwise. Many examples are found in the literature (Walker et al., 1997; Williams, 2006; Peng
et al., 2017). Other variants have also been used, such as those to measure partial success (Singh
et al., 2002; Young et al., 2016).
The second class measures cost incurred in a dialogue, such as time elapsed. A simple yet useful
example is the number of turns, which reflects the intuition that a more succinct dialogue is preferred
with everything else being equal. The reward is simply ÙÄÄÄ1 per turn, although more complicated
choices exist (Walker et al., 1997).
In addition, other aspects of dialogue quality may also be encoded into the reward function, although
this is a relatively under-investigated direction. In the context of chatbots (Chapter 5), coherence, diversity
and personal styles have been used to result in more human-like dialogues (Li et al., 2016a,b).
They can be useful for task-oriented dialogues as well. In Sec. 4.4.6, we will review a few recent
works that aim to learn reward functions automatically from data.


Agenda-Based Simulation. As an example, we describe a popular hidden agenda-based user simulator
developed by Schatzmann and Young (2009), as instantiated in Li et al. (2016d) and Ultes
et al. (2017c). Each dialogue simulation starts with a randomly generated user goal that is unknown
to the dialogue manager. In general the user goal consists of two parts: the inform-slots contain
a number of slot-value pairs that serve as constraints the user wants to impose on the dialogue; the
request-slots are slots whose values are initially unknown to the user and will be filled out during
the conversation. Fig. 4.2 shows an example user goal in a movie domain, in which the user is trying
to buy 3 tickets for tomorrow for the movie batman vs. superman.
Furthermore, to make the user goal more realistic, domain-specific constraints are added, so that
certain slots are required to appear in the user goal. For instance, it makes sense to require a user to
know the number of tickets she wants in the movie domain.
During the course of a dialogue, the simulated user maintains a stack data structure known as user
agenda. Each entry in the agenda corresponds to a pending intention the user aims to achieve, and
their priorities are implicitly determined by the first-in-last-out operations of the agenda stack. In
other words, the agenda provides a convenient way of encoding the history of conversation and the
‚Äústate-of-mind‚Äù of the user. Simulation of a user boils down to how to maintain the agenda after
each turn of the dialogue, when more information is revealed. Machine learning or expert-defined
rules can be used to set parameters in the stack-update process.

Model-based Simulation. Another approach to building user simulators is entirely based on
data (Eckert et al., 1997; Levin et al., 2000; Chandramohan et al., 2011). Here, we describe a
recent example due to El Asri et al. (2016). Similar to the agenda-based approach, the simulator
also starts an episode with a randomly generated user goal and constraints. These are fixed during a
conversation.
In each turn, the user model takes as input a sequence of contexts collected so far in the conversation,
and outputs the next action. Specifically, the context at a turn of conversation consists of:
 the most recent machine action,
 inconsistency between machine information and user goal,
39
 constraint status, and
 request status.
With these contexts, an LSTM or other sequence-to-sequence models are used to output the next
user utterance. The model can be learned from human-human dialogue corpora. In practice, it often
works well by combining both rule-based and model-based techniques to create user simulators.

building a human-like simulator remains challenging. In fact, even user simulator evaluation itself
continues to be an ongoing research topic (Williams, 2008; Ai and Litman, 2008; Pietquin and
Hastie, 2013). In practice, it is often observed that dialogue policies that are overfitted to a particular
user simulator may not work well when serving another user simulator or real humans (Schatzmann
et al., 2005b; Dhingra et al., 2017). The gap between a user simulator and humans is the major
limitation of user simulation-based dialogue policy optimization.
Some user simulators are publicly available for research purposes. Other than the aforementioned
agenda-based simulators by Li et al. (2016d); Ultes et al. (2017c), a large corpus with an evaluation
environment, called AirDialogue (in the flight booking domain), was recently made available (Wei
et al., 2018). At the IEEE workshop on Spoken Language Technology in 2018, Microsoft organized
a dialogue challenge1 of building end-to-end task-oriented dialogue systems by providing an
experiment platform with built-in user simulators in several domains (Li et al., 2018).


Chapter 2
Evaluating Conversational
Information Retrieval
A key challenge in CIR is evaluation, to compare different CIR designs and approaches, and carefully
consider the different forms of evaluation to draw conclusions on what works best. In each turn of a
conversation, the system updates dialog state and performs action selection, choosing what to present
from a variety of action responses (Figure 1.4). Some actions involve retrieval of data types such as
phrases, passages, documents, knowledge base entities, and multimedia. There are also non-retrieval
actions, such as initiating chit chat, recommending follow-up questions that the user might ask, and
asking the user a clarifying question.
The user‚Äôs satisfaction with this iterative process is our primary concern. How do we know whether
the conversational system is working well? Is it selecting the right actions? Is each action producing
optimal outputs? Does the overall approach give the user a positive, useful, and friction-free CIR
experience?

System-oriented Evaluation
System-oriented evaluation involves using a fixed dataset that captures the input and desired output
of the retrieval system or one of its components. First we present examples of evaluation without
conversational context. Then we show how these can be adapted to capture conversational context,
and list a number of example datasets. Finally we describe system-oriented evaluation of non-retrieval
components.
2.2.1 Evaluating retrieval
For some components of a CIR system (Figure 1.4), system-oriented evaluation can be carried out by
considering the current request without any context, as though it were a user‚Äôs first request. Such
evaluation can be useful because handing a single request is a capability that any retrieval system
should have. Two examples of this kind of retrieval evaluation without context are a document search
(action) module, which can be evaluated using an IR test collection dataset, and a result generation
module, which can be evaluated using a reading comprehension dataset.
System-oriented evaluation of a document search component is the same as evaluation with a
traditional information retrieval test collection. A test collection comprises a set of documents, a
set of user queries, and ground truth labels indicating query-document relevance [Voorhees et al.,
2005]. In a typical research test collection the document corpus is fixed, and the retrieval system is

expected to produce a ranking of documents for each query, with the goal of having the most relevant
documents at the top of the list.
If there are multiple grades of relevance (irrelevant, partially relevant, relevant, highly relevant)
and multiple items in the ranked list, there are a variety of metrics which can be used to generate
a list-level measure of quality. Here we choose one well-known example, which is Normalized
Discounted Cumulative Gain (NDCG). The metric is Cumulative because it sums over rank positions.
The Discount is based on the rank ùëñ of the document, giving more importance to documents at the top
of the ranking, since users will see those first. The Gain is based on the relevance judgment the ùëñth
result ùê∫¬πùëüùëíùëôùëñ¬∫, indicating how valuable the document is for the user. This gives us NDCG:

The Normalizer is a value ùëç that means a perfect ranking, that is ranked in descending order of Gain,
has NDCG@ùëò = 1. Like most IR metrics, NDCG rewards the system for retrieving results with the
most valuable results at the highest ranks.
The magnitude of mean NDCG and other IR metrics can vary depending on the application setting
and query sample. Rather than focusing on magnitude, studies usually calculate NDCG for two
systems on the same set of queries. Their performance is typically compared using a paired statistical
significance test, to compare mean NDCG.
System-oriented evaluation of a result generation component can be thought of as evaluation of a
machine reading comprehension system (Chapter 5), to see how well the system can generate or
extract an answer from a retrieved paragraph of text. Take extractive readers (Section 5.2) as an
example. Each instance in the evaluation considers a particular question and paragraph, capturing the
user‚Äôs required answer by identifying one or more spans of paragraph text that answer the question
[Rajpurkar et al., 2016]. Identifying multiple spans is useful to identify the human level of agreement
on the task, and to capture that there can be multiple possible correct spans.
The system sees a different paragraph for each test instance, modeling a situation where some upstream
retrieval module has already been applied to find a candidate paragraph. The system identifies a span
of text, and it can be evaluated using two metrics
‚Ä¢ Exact match: The proportion of questions where the predicted span is an exact match for a
ground truth span.
‚Ä¢ F1 score: Calculates the F1 score, which is the harmonic mean of precision and recall. The
precision is the proportion of predicted span tokens that are in the ground truth span, and
recall is the proportion of ground truth span tokens that are in the predicted span. For a
question with multiple ground truth spans we take the maximum F1.
This kind of evaluation is also used in a comparative fashion, for example to see if a new approach
has (significantly) better performance than a baseline.

These examples of system-oriented datasets, for document retrieval and result generation, have a
common structure. They supply some content, a query, and some annotations indicating what content
answers the query. It is then possible to evaluate two or more systems using the same fixed dataset,
identifying which system had query responses that better match the ground truth annotation. The
advantage of system-oriented evaluation is that it is possible to reuse the dataset across many different
experiments and studies. One disadvantage is that it is not guaranteed that the dataset captures a
valid real-world application. If the content, query or annotation fails to capture the requests and
preferences of users on a real application, results on the dataset may tell us very little about the true
quality of the systems being evaluated. Another disadvantage is that reuse of the dataset may lead
to false conclusions via overfitting, where the same test data was used multiple times by the same
researcher, leading the researcher to inadvertently make decisions using the test data. One way to
avoid overfitting is to limit the number of iterations by having a leaderboard where each submission is
publicly tracked, as in Rajpurkar et al. [2016]. Another is to have a single-shot evaluation where the
test annotations are not even made public until after all submissions are finalized, as in Voorhees
et al. [2005]. Another is to require that published studies evaluate the same approaches over multiple
datasets without per-dataset iteration.

We have seen how to evaluate document retrieval and answer extraction, which could be actions in
a CIR system, or sub-components of an action that combines retrieval and extraction. Improving
these could lead to an overall improvement in the quality of CIR results. However, the datasets we
considered so far are based on a single self-contained query request. In a conversational setting, the
query request happens in the context of an ongoing multi-turn interaction. The CIR system can make
use of the past requests and responses in generating its response for the current request, in a way that
brings its response more in line with the current user needs, making the response more appropriate
given the ongoing conversation.
This leads to CIR-specific datasets, which incorporate conversational context. One example of this is
the TREC Conversational Assistant Track (CAsT) [Dalton et al., 2020], which is an IR test collection
with a corpus, queries and labels. However, when we process the query in CAsT, we now know what
happened in previous turns of the conversation. The labeling also takes this context into account. For
example, if the current query is ‚ÄúWhat is different compared to previous legislation?‚Äù the previous
query may be ‚ÄúWhat is the purpose of GDPR?‚Äù then both the retrieval system and the labeling must
take into account the previous query. The system is rewarded for finding information on how GDPR
compares to previous legislation.
Relevance judgments for the current query take this conversational context into account. This means
that evaluation using a metric such as NDCG is evaluating the system‚Äôs ability to make appropriate
use of context when ranking. We note, although it evaluates the system in conversational context,
the previous steps of the conversation are shared for all systems being evaluated, even though each
system may have dealt with the previous steps differently. Each query is like a branching point, after
some predetermined conversation steps, and NDCG tells us whether the first branching step had good
results. This makes it off-policy evaluation. We can compare this to on-policy evaluation, where a
system is deployed, and we test its ability to use its own past behavior to improve performance. One
way of understanding such effects is to evaluate with real users, as we will describe later in the chapter.
The previous section described a typical non-conversational dataset as having three main aspects:
The query, the content and the annotation of which content is correct for the query. This section
describes how the query may be interpreted and annotated in a conversational context. The main
difference is that we would now describe a dataset as having: query+context, content and annotation.
The annotation, and potentially the content selection, can now take the context into account. Table 2.2
summarizes a number of benchmarks and other datasets in this way. The non-conversational IR test
collection approach is exemplified by the MS MARCO passage dataset [Bajaj et al., 2016]. The corpus
contains 8.8 million passages, the queries do not have any conversational context, and the annotations
are passage-level relevance judgments with respect to a query. The answer extraction approach we
described is SQuAD [Rajpurkar et al., 2016], which has no special conversational context and is
evaluated based on identifying correct spans from a paragraph. We also described TREC CaST, which
is related to MS MARCO, but adds the conversational context. This makes it much more suitable for
CIR evaluation, since we have argued that user revealment over multiple steps of conversation is an
important property of a CIR process.
The section of Table 2.2 about finding answers lists several other kinds of dataset that are worth
noting, several of which are also described in other parts of the book. The content given in some
cases is a paragraph of text, which means we assume some process has already taken place to find a
paragraph of interest to the user. In other cases, the content is structured data, such as Wikipedia
tables, Wikidata knowledge triples and databases. We also see cases where the content is a text
corpus, so we are either searching a corpus of full documents or a corpus of shorter passages of text.
In all cases the annotated output is shorter than a full document, reflecting the goal of a conversational
system to identify outputs that are short enough for presentation to the user on a small screen or
through voice. The answer can be a passage of text such as a sentence, or it can even be a span of
text that gives a direct answer. Some datasets identify both the passage and the direct answer. In
a real conversational search system, users might be interested in several levels of granularity, for
example highlighting a two-word answer on the screen, while also showing the containing passage
and providing a link to the document that contains the passage. Users can check the document to
justify the answer that was found, and in general may be more comfortable if the system provides easy
access to such information.

CIR user simulation. We have seen that evaluation using a fixed dataset can capture conversational
context, although the CIR system is effectively jumping into a conversational context, where the
previous steps were carried out by some other CIR system. Interactive studies with real users do not
have this problem, but raise the issue of either running large-scale A/B tests, which is not available to
every academic, or running a lab or field study, which is expensive to do. A third form of evaluation
may emerge in the coming years, of evaluating with fixed data rather than a real user, but with a
simulated user that can carry out multiple conversational steps.
Although it is not common to evaluate CIR using user simulation, in related areas of conversational
AI there are already user simulation methods. As summarized by Gao et al. [2019] simulators for
dialog agents can be based on randomly generated scenarios. A scenario has two parts. One is the
things that the user does not yet know, for example to book a movie ticket, the user wants to find
out the ticket, theater and start time. The other is the user‚Äôs constraints, that she wants to book for
three people, tomorrow and for the movie ‚Äúbatman vs superman‚Äù. The simulation can then be carried
out using a rule-based approach, where the simulator represents the users ‚Äústate of mind‚Äù using a
stack data structure known as the user agenda. The simulation can also use a model-based approach,
which still has the user‚Äôs goal and constraints, but trains a sequence-to-sequence model for simulation.
Simulators, particularly rule-based ones, can incorporate domain knowledge in the process. For
example, the user must always have a number of people in mind, and that number of people should be
greater than zero but not too large.
Taking the simulation approach from dialog systems, it is possible to adapt it to conversational
recommendation [Zhang and Balog, 2020]. Building on the agenda-based approach, the simulated
user maintains a stack of requests, and will select between pulling and pushing (replacing) on the
stack depending on whether the user action has been met with an appropriate response. The user
actions are disclose, reveal, inquire, navigate, note and complete. The user‚Äôs preferences are modeled
via a set of known preferred items and a knowledge graph that allows generalization to related items.
Natural language understanding is based on entity linking to understand what the conversational
recommendation system is talking about, and also based on the assumption that the user can understand
which of the actions the agent is carrying out, based on some training data based on past agent
behavior with action annotation. Natural language generation is based on a small set of templates.
It is likely that these simulation approaches for dialog systems and conversational recommendation
can be adapted to CIR. The difference is the lack of structure, due to the free-form nature of IR
information needs. The information need underlying a CIR session could be to book a movie ticket
at a particular time for a particular number of people, like in the dialog system. It could also be to
recommend a movie, given the preferences of a user across known movies and their knowledge graph
attributes. The CIR system could help the user visit a movie booking site or search for documents
about movie recommendations. But the CIR user‚Äôs need may also be to learn about a movie director
Stanley Kubrik‚Äôs preparation process for a feature film, which would take on average four years. If
there are a great variety of information needs, the CIR system needs to be open-domain, answering
needs that do not fit into a dialog slot-filling paradigm, or a knowledge graph recommendation. We
hope to see more progress in open-domain user simulation in the coming years.

Responsible CIR. Since search and recommendation systems can make the difference between users
discovering content and never seeing it at all, recent papers have considered the ethical concerns of such
retrieval systems. One consideration is whether the IR system is systematically under-representing
certain types of content [Singh and Joachims, 2018; Biega et al., 2018], such as mostly showing male
CEOs for the query ‚Äúceo‚Äù, under-representing female CEOs. Another consideration is that the IR
system may not perform equally well for all groups of users, perhaps providing lower quality search
results to some group [Mehrotra et al., 2017] because it is smaller and contributes less to the overall
training data of the IR system.
Such considerations are an emerging topic in online systems for search and recommendation, and
should be also considered in the context of CIR. The problems may be similar in conversational and
non-conversational systems. For example, the concerns of under-represented content and under-served
users, described in the previous paragraph, are also likely to arise in conversational systems. In all
types of system, we should be concerned with protecting users from potentially harmful results such
as scams or disinformation. The Alexa Prize [Ram et al., 2018a], focused on conversational assistants,
identified a related set of inappropriate response types: Profanity, sexual responses, racially offensive
responses, hate speech, insulting responses, and violent responses.

In the Xiaoice social chatbot [Zhou et al., 2020] a number of areas were identified for ongoing study.
Privacy is a concern, since users reveal information about themselves to the conversational system.
This is certainly true inWeb search as well, where users reveal their thoughts to the engine in the form
of queries, but could happen even more if the conversation with the agent elicits further details or
even (in the case of a chatbot) is treated as a friend. Another concern is allowing Xiaoice to suggest
not conversing or even refuse a conversation, either because it is late at night or because the topic
is inappropriate. Another concern is that the system should make it clear what it can and cannot
do, admitting that it is fallible, because users who trust the system too much could make a mistake.
The overall concern is how all the possible impacts on human users can be correctly evaluated and
optimized in a machine learning based system.


Question Suggestions in Commercial Search Engines
As commercial search engines are getting significantly better in powering natural language based
search, many users, however, still tend to issue single-turn keyword queries after years of experience
with search engine‚Äôs failures on supporting multi-turn natural language questions.
To encourage users to ask more conversational questions, many commercial search engines (e.g.,
Google and Bing) have been presenting the People Also Ask (PAA) feature, which aims to proactively
engage users in conversation-like experiences by suggesting natural language questions. Instead of
addressing a user‚Äôs current information needs, PAA suggests questions users would be interested in
for the next step of their inquiry.
Figure 7.4 shows an example of the PAA suggestion panel from Bing. Users can click on a suggested
question to reveal its answer and the PAA panel can expand with more question suggestions based
on the clicked question. The design of PAA is based on the assumption that the current information
need is often addressed in organic results, thus one way of creating a conversational experience is to
provide interesting question suggestions that a user is likely to follow to continue the search. For
example, the targeted question suggestions for ‚ÄúNissan GTR Price‚Äù (Figure 7.4) include those that
help user complete a task (‚Äúleasing deals‚Äù), weigh options (‚Äúpros and cons about GTR‚Äù), explore a
related topic (‚Äúultimate streetcar‚Äù), or learn more details (‚Äú2020 GTR‚Äù). These question suggestions
are proactive in that they lead the user to an interactive search experience with related and diverse
future search results.

Conversational Search Engine
With the advances in CIR, even standard GUI interfaces to search have become ‚Äúmore conversational‚Äù.
These changes can be seen as part of a broader trend acknowledging that a search engine site will
continue to be useful in many contexts where voice-based search is inconvenient; however, it may still
be desirable to introduce analogous affordances on a Search Engine site for conversational techniques
for disambiguation, suggesting new topics, etc. Such trends are evidenced on a number of major search
engines including Google and Bing as well as other search engines. Since some of the technology
behind these capabilities have been discussed in earlier chapters. We focus on how these capabilities
present in the user experience and how the capabilities relate to a conversational style.
Figure 8.8 shows a screenshot of Microsoft‚Äôs Bing Search Engine in response to the query ‚Äúis the
ford mustang fast‚Äù. The inset box provides an aggregate instant answer (‚ÄúYes‚Äù) with perspectives
justifying the answer (‚Äútwo source‚Äù). The same capability can be used to present perspectives which
disagree on more subjective questions. On the right side, there is an entity pane with ‚Äúattributes‚Äù that
lead to new topics to explore. This entity pane has become quite common in many search engines
and can be thought of as a similar approach to XiaoIce‚Äôs strategy to create engagement with novel
contextually relevant information. Another conversational capability seen in the same screenshot, is
the ‚ÄúPeople Also Ask‚Äù feature which focuses on suggesting new useful questions focused on natural
follow-up questions under the assumption that the instant answer or other results will answer the main
query. For an in-depth discussion of techniques which can power such experiences, the reader should
see Section 7.2.
To illustrate another conversational capability, Figure 8.9 shows a screenshot of Bing in response to
the query ‚Äúconvert string to int‚Äù. Note that here the search engine has responded with a clarification
question [Zamani et al., 2020a,c] ‚ÄúWhat programming language are you looking for?‚Äù with a number
of options. Beyond simply being a clarification question in a more conversational style, the use of
‚Äúprogramming language‚Äù is again an example of a similar technique used in chatbots like XiaoIce. By
introducing external knowledge (the query never mentioned ‚Äúprogramming language‚Äù), the search
engine introduces new information into the conversation rather than using a generic prompt such as
‚Äúwhich of these would you like?‚Äù Similar to the studies on chatbots, Zamani et al. [2020c] provide
evidence that the more specific question leads to higher engagement than the more general question.
The reader are referred to Section 7.1 for an in-depth discussion of techniques.
Search engines are continuously experimenting with such features. Some may only last a short time
as not all conversational techniques work as well in a website as a chatbot or voice assistant, other
changes become longer lasting. One research challenge that still persists in this space is to make
search engines more contextual across multiple queries and answers in a search session. This is a
case where the technology to power such experiences in chatbots and voice-assistants have evolved
well beyond the web experience. Nonetheless, we believe that search engines will continue this trend
of evolving to become more conversational and contextual.
